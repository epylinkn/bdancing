{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /anaconda3/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /anaconda3/lib/python3.6/site-packages (from keras) (1.0.6)\n",
      "Requirement already satisfied: six>=1.9.0 in /anaconda3/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: h5py in /anaconda3/lib/python3.6/site-packages (from keras) (2.7.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /anaconda3/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda3/lib/python3.6/site-packages (from keras) (1.0.5)\n",
      "Requirement already satisfied: pyyaml in /anaconda3/lib/python3.6/site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /anaconda3/lib/python3.6/site-packages (from keras) (1.14.3)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 1s 511us/step - loss: 0.7030 - acc: 0.5220\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 92us/step - loss: 0.6905 - acc: 0.5340\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 91us/step - loss: 0.6871 - acc: 0.5500\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 85us/step - loss: 0.6796 - acc: 0.5520\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 114us/step - loss: 0.6764 - acc: 0.5730\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 90us/step - loss: 0.6763 - acc: 0.5720\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 89us/step - loss: 0.6719 - acc: 0.5840\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 138us/step - loss: 0.6669 - acc: 0.5930\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 113us/step - loss: 0.6663 - acc: 0.5840\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 136us/step - loss: 0.6596 - acc: 0.6020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1106d0898>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a single-input model with 2 classes (binary classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 11.4611 - acc: 0.0880 - val_loss: 11.3269 - val_acc: 0.0900\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 11.4593 - acc: 0.1080 - val_loss: 11.3269 - val_acc: 0.0800\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 11.4588 - acc: 0.1130 - val_loss: 11.3238 - val_acc: 0.1100\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 11.4582 - acc: 0.1050 - val_loss: 11.3236 - val_acc: 0.1000\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 11.4576 - acc: 0.1040 - val_loss: 11.3224 - val_acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.09817108, 0.09967864, 0.10043235, 0.09956948, 0.10148516,\n",
       "        0.10239964, 0.10192678, 0.09888098, 0.09927654, 0.0981794 ],\n",
       "       [0.1011415 , 0.10128884, 0.09961907, 0.09810194, 0.10107305,\n",
       "        0.10313573, 0.101903  , 0.0978321 , 0.10055922, 0.09534549],\n",
       "       [0.09945913, 0.10096081, 0.10050876, 0.09817161, 0.10130619,\n",
       "        0.10278415, 0.1017039 , 0.09915031, 0.09927876, 0.09667639],\n",
       "       [0.09886111, 0.10051115, 0.10029706, 0.10009791, 0.10006414,\n",
       "        0.10295076, 0.10205568, 0.09864096, 0.09952211, 0.09699917],\n",
       "       [0.09793415, 0.10080733, 0.10080915, 0.09998483, 0.10002817,\n",
       "        0.10368074, 0.10246438, 0.09832136, 0.09975689, 0.09621301],\n",
       "       [0.10090615, 0.10026655, 0.09905529, 0.09966928, 0.10123424,\n",
       "        0.10273839, 0.10089646, 0.09918346, 0.0993709 , 0.09667936],\n",
       "       [0.09858765, 0.09975594, 0.10059717, 0.10155801, 0.09960083,\n",
       "        0.10205176, 0.10250962, 0.09927712, 0.09836716, 0.09769469],\n",
       "       [0.10008316, 0.09954283, 0.10011081, 0.10034056, 0.10018594,\n",
       "        0.10076832, 0.10225818, 0.09917469, 0.09893614, 0.09859936],\n",
       "       [0.09986629, 0.10138492, 0.09952407, 0.098828  , 0.1018291 ,\n",
       "        0.10352872, 0.10130501, 0.09845157, 0.10025346, 0.0950288 ],\n",
       "       [0.09937093, 0.10108404, 0.10030322, 0.09811661, 0.10137525,\n",
       "        0.10344093, 0.10287759, 0.09734315, 0.10098156, 0.09510676],\n",
       "       [0.10048464, 0.10086194, 0.09946439, 0.09880524, 0.10111558,\n",
       "        0.10451348, 0.10101365, 0.09793182, 0.10056513, 0.09524418],\n",
       "       [0.1018721 , 0.09993597, 0.09940157, 0.09961376, 0.1005721 ,\n",
       "        0.10111784, 0.10217234, 0.09901933, 0.09957534, 0.09671956],\n",
       "       [0.09881804, 0.10157248, 0.10093294, 0.0991215 , 0.10079803,\n",
       "        0.10286645, 0.10225227, 0.09912053, 0.09782345, 0.09669434],\n",
       "       [0.09825919, 0.10092804, 0.10026511, 0.09862852, 0.10149996,\n",
       "        0.10418765, 0.10078963, 0.09949044, 0.09956657, 0.09638492],\n",
       "       [0.10146913, 0.10113475, 0.09978982, 0.09854713, 0.10033524,\n",
       "        0.10286022, 0.101211  , 0.09866426, 0.09985544, 0.09613299],\n",
       "       [0.10023332, 0.10060213, 0.10035484, 0.09753516, 0.10193215,\n",
       "        0.1026575 , 0.10185249, 0.09820777, 0.10057575, 0.09604881],\n",
       "       [0.09976307, 0.09930167, 0.10038386, 0.10116015, 0.10027318,\n",
       "        0.10076219, 0.10277165, 0.09937859, 0.09805273, 0.098153  ],\n",
       "       [0.09980098, 0.10030317, 0.0998039 , 0.09979044, 0.10073491,\n",
       "        0.10302372, 0.10254914, 0.09806579, 0.09953111, 0.0963968 ],\n",
       "       [0.10129751, 0.0997663 , 0.09916395, 0.10026801, 0.1001753 ,\n",
       "        0.10204993, 0.10172432, 0.09879262, 0.09958903, 0.09717308],\n",
       "       [0.09835307, 0.10109997, 0.1004857 , 0.09975337, 0.10117766,\n",
       "        0.10394139, 0.10135138, 0.09873871, 0.09858281, 0.09651594],\n",
       "       [0.09886161, 0.10067263, 0.0998352 , 0.09899716, 0.1015441 ,\n",
       "        0.10374081, 0.10134145, 0.09894864, 0.10016367, 0.09589483],\n",
       "       [0.09878002, 0.10056936, 0.10132129, 0.09919739, 0.10003991,\n",
       "        0.10188818, 0.10289531, 0.09884155, 0.09838533, 0.09808163],\n",
       "       [0.09886561, 0.10057245, 0.10056472, 0.09828572, 0.10135895,\n",
       "        0.10314388, 0.10179649, 0.09833413, 0.09999208, 0.09708603],\n",
       "       [0.10086087, 0.09954118, 0.0992512 , 0.10034894, 0.10049437,\n",
       "        0.10162356, 0.10204285, 0.09847192, 0.09980097, 0.09756406],\n",
       "       [0.0983833 , 0.09976554, 0.10031213, 0.10037884, 0.10113952,\n",
       "        0.10272004, 0.10265771, 0.09755187, 0.10027935, 0.0968117 ],\n",
       "       [0.09961825, 0.10031903, 0.10024689, 0.09901338, 0.10077839,\n",
       "        0.10269792, 0.10125974, 0.09917943, 0.09905053, 0.09783646],\n",
       "       [0.10063584, 0.09996528, 0.09942157, 0.0992431 , 0.1009963 ,\n",
       "        0.1033845 , 0.10119624, 0.09850845, 0.10075916, 0.09588954],\n",
       "       [0.09929804, 0.10168934, 0.10100599, 0.09732082, 0.10181579,\n",
       "        0.10350083, 0.10177625, 0.09795734, 0.09974694, 0.09588864],\n",
       "       [0.10100836, 0.09918915, 0.09975274, 0.10127138, 0.09901895,\n",
       "        0.10131576, 0.1027349 , 0.09886941, 0.09934484, 0.0974945 ],\n",
       "       [0.09961861, 0.10066529, 0.10048728, 0.09866738, 0.10128938,\n",
       "        0.1026754 , 0.10179421, 0.09845899, 0.09912997, 0.09721348],\n",
       "       [0.10070025, 0.10116448, 0.09935923, 0.0993943 , 0.10185857,\n",
       "        0.1036945 , 0.10098238, 0.09839152, 0.10003736, 0.09441738],\n",
       "       [0.10034286, 0.09992456, 0.09951098, 0.10003063, 0.10081058,\n",
       "        0.10235764, 0.10185789, 0.09892955, 0.10003656, 0.0961987 ],\n",
       "       [0.09998728, 0.10110799, 0.10018128, 0.09831715, 0.10144147,\n",
       "        0.10284255, 0.10169109, 0.09887646, 0.09876241, 0.09679233],\n",
       "       [0.10025999, 0.10128944, 0.10023865, 0.09904754, 0.10007557,\n",
       "        0.10359606, 0.10221507, 0.09823918, 0.10024942, 0.09478913],\n",
       "       [0.09864441, 0.09991399, 0.10062591, 0.10049611, 0.09961215,\n",
       "        0.10288458, 0.10325684, 0.09772217, 0.10055719, 0.0962866 ],\n",
       "       [0.10027665, 0.10054891, 0.09954685, 0.10009025, 0.10061939,\n",
       "        0.10336572, 0.10182482, 0.09816732, 0.09974454, 0.09581558],\n",
       "       [0.0986791 , 0.1012447 , 0.10102951, 0.09835514, 0.10118297,\n",
       "        0.1031739 , 0.10141837, 0.10012452, 0.09859424, 0.09619751],\n",
       "       [0.09887479, 0.10125983, 0.10083763, 0.09881362, 0.10077078,\n",
       "        0.103311  , 0.10114264, 0.09934306, 0.09843134, 0.09721529],\n",
       "       [0.10070115, 0.09998944, 0.10012446, 0.09927966, 0.10041367,\n",
       "        0.10181759, 0.10184806, 0.09857152, 0.09954138, 0.09771312],\n",
       "       [0.10027629, 0.10135173, 0.10013235, 0.09815419, 0.10136033,\n",
       "        0.10351013, 0.10138246, 0.09895096, 0.09886873, 0.09601283],\n",
       "       [0.10142767, 0.10014158, 0.09939085, 0.09934705, 0.10062985,\n",
       "        0.10226854, 0.10158715, 0.09831436, 0.10008149, 0.09681149],\n",
       "       [0.09983736, 0.10018427, 0.10003685, 0.09868241, 0.10137608,\n",
       "        0.10367607, 0.10100368, 0.09823936, 0.10051097, 0.09645297],\n",
       "       [0.10053375, 0.10067105, 0.10001255, 0.09856887, 0.10051814,\n",
       "        0.10332926, 0.10117708, 0.09927128, 0.09948655, 0.09643146],\n",
       "       [0.10033546, 0.1003725 , 0.10020266, 0.09799678, 0.10208928,\n",
       "        0.10169357, 0.10097861, 0.10007537, 0.09877724, 0.09747854],\n",
       "       [0.09948832, 0.1008227 , 0.10019132, 0.09917314, 0.10103386,\n",
       "        0.10229997, 0.1017774 , 0.09950888, 0.09798782, 0.09771658],\n",
       "       [0.10056798, 0.09992813, 0.09944687, 0.10044239, 0.10065091,\n",
       "        0.10149355, 0.10236166, 0.098436  , 0.09910908, 0.09756351],\n",
       "       [0.10028384, 0.10021137, 0.09983186, 0.09867768, 0.10211576,\n",
       "        0.10316984, 0.10119161, 0.09883338, 0.10027947, 0.09540518],\n",
       "       [0.09786125, 0.10033994, 0.10132045, 0.1001415 , 0.09974544,\n",
       "        0.10235053, 0.10304338, 0.09827423, 0.09896602, 0.09795729],\n",
       "       [0.10102233, 0.09906182, 0.09926066, 0.10081656, 0.09989933,\n",
       "        0.10075364, 0.10363204, 0.09760401, 0.10021432, 0.09773534],\n",
       "       [0.09779146, 0.09964919, 0.10049424, 0.09920187, 0.10161257,\n",
       "        0.10346267, 0.10142416, 0.09887172, 0.10025609, 0.09723608],\n",
       "       [0.09987253, 0.1004244 , 0.09972788, 0.0992438 , 0.10101155,\n",
       "        0.10279278, 0.10165689, 0.0990021 , 0.10040073, 0.09586734],\n",
       "       [0.09900332, 0.10118354, 0.10070995, 0.098732  , 0.10131148,\n",
       "        0.10323816, 0.10124203, 0.09944235, 0.09911104, 0.09602618],\n",
       "       [0.10073414, 0.09962789, 0.09907087, 0.09984849, 0.10113049,\n",
       "        0.10252646, 0.10139507, 0.0994232 , 0.09933117, 0.09691219],\n",
       "       [0.09863517, 0.10095022, 0.10168437, 0.09929469, 0.09961335,\n",
       "        0.1015774 , 0.10408157, 0.09751166, 0.09935785, 0.09729376],\n",
       "       [0.10018466, 0.10002709, 0.09999494, 0.09973165, 0.09992199,\n",
       "        0.10222817, 0.10205657, 0.09858503, 0.09963765, 0.0976323 ],\n",
       "       [0.09980375, 0.10120998, 0.10063773, 0.09788901, 0.10038598,\n",
       "        0.10332946, 0.1022414 , 0.09820978, 0.09980069, 0.09649219],\n",
       "       [0.10137009, 0.1004063 , 0.10000818, 0.09910384, 0.10026784,\n",
       "        0.10224685, 0.10198276, 0.09817042, 0.10003719, 0.09640655],\n",
       "       [0.09949493, 0.09978516, 0.1005377 , 0.09956319, 0.10094696,\n",
       "        0.1012236 , 0.10253357, 0.09823012, 0.09898845, 0.09869628],\n",
       "       [0.10042979, 0.10030723, 0.09996437, 0.09912974, 0.10147148,\n",
       "        0.1021423 , 0.1004822 , 0.09999193, 0.09843152, 0.09764943],\n",
       "       [0.10123479, 0.09973074, 0.09938902, 0.09979339, 0.10060648,\n",
       "        0.10196026, 0.10282023, 0.09733593, 0.09997943, 0.09714974],\n",
       "       [0.10034851, 0.10091712, 0.1001735 , 0.09975361, 0.10031185,\n",
       "        0.10223619, 0.10226388, 0.09837816, 0.0986675 , 0.09694966],\n",
       "       [0.0983559 , 0.10028954, 0.10048391, 0.10056626, 0.10011755,\n",
       "        0.10249518, 0.10173566, 0.09967084, 0.09846749, 0.09781764],\n",
       "       [0.09993247, 0.10039005, 0.10015652, 0.1003978 , 0.09963291,\n",
       "        0.10200561, 0.10202018, 0.09939425, 0.09958433, 0.09648588],\n",
       "       [0.10041281, 0.09970807, 0.10009246, 0.10042106, 0.10032921,\n",
       "        0.10118762, 0.10235976, 0.09906276, 0.098729  , 0.09769718],\n",
       "       [0.10046121, 0.09953294, 0.09947519, 0.10088929, 0.10015931,\n",
       "        0.1021284 , 0.10274111, 0.09736252, 0.10057776, 0.09667227],\n",
       "       [0.10079843, 0.10033123, 0.09985496, 0.09888697, 0.10155644,\n",
       "        0.10211171, 0.10162637, 0.09899614, 0.0996979 , 0.0961398 ],\n",
       "       [0.10091232, 0.09957808, 0.09927764, 0.09969779, 0.10071994,\n",
       "        0.10145889, 0.10272314, 0.09859662, 0.10040642, 0.0966291 ],\n",
       "       [0.10060658, 0.09981892, 0.09954379, 0.1010836 , 0.09944289,\n",
       "        0.10117637, 0.10277122, 0.09901552, 0.09912372, 0.09741731],\n",
       "       [0.09976751, 0.10089175, 0.10045422, 0.09945945, 0.09976937,\n",
       "        0.10274576, 0.10292779, 0.09776297, 0.0996241 , 0.09659711],\n",
       "       [0.1000794 , 0.09922766, 0.0997242 , 0.10099227, 0.10066979,\n",
       "        0.10207353, 0.10134664, 0.09946776, 0.09983682, 0.09658188],\n",
       "       [0.09858086, 0.10065698, 0.10116617, 0.09968394, 0.10016423,\n",
       "        0.10251426, 0.10223307, 0.09861488, 0.0982325 , 0.09815308],\n",
       "       [0.09915775, 0.10128058, 0.10042992, 0.09911081, 0.10161902,\n",
       "        0.10252749, 0.10094071, 0.09904125, 0.09919913, 0.09669341],\n",
       "       [0.09935263, 0.10018823, 0.10018154, 0.09974022, 0.10038767,\n",
       "        0.10194697, 0.10276479, 0.09895156, 0.09929612, 0.09719028],\n",
       "       [0.09998139, 0.10045717, 0.10036016, 0.0989105 , 0.10017245,\n",
       "        0.10244672, 0.10209242, 0.09837154, 0.10016564, 0.09704202],\n",
       "       [0.09975379, 0.09929217, 0.10018321, 0.10028376, 0.10026906,\n",
       "        0.10238625, 0.10204429, 0.09892807, 0.09999807, 0.09686131],\n",
       "       [0.09923405, 0.09940106, 0.10080382, 0.09941322, 0.09979805,\n",
       "        0.10169142, 0.10299546, 0.09846315, 0.09962065, 0.09857912],\n",
       "       [0.10092521, 0.10015275, 0.09976762, 0.09874241, 0.10134962,\n",
       "        0.10163012, 0.10156537, 0.09906946, 0.09980193, 0.09699547],\n",
       "       [0.09966902, 0.09995732, 0.0994855 , 0.09978362, 0.10119413,\n",
       "        0.1025983 , 0.10128083, 0.09888724, 0.09992627, 0.09721772],\n",
       "       [0.10090075, 0.1001267 , 0.09893326, 0.0994821 , 0.1010795 ,\n",
       "        0.1034324 , 0.10195826, 0.0977715 , 0.10068279, 0.09563275],\n",
       "       [0.09968691, 0.10112999, 0.10001046, 0.10006519, 0.09968995,\n",
       "        0.10426152, 0.10195552, 0.09808262, 0.09961835, 0.09549947],\n",
       "       [0.10000133, 0.10035763, 0.09998843, 0.10019086, 0.10041646,\n",
       "        0.10180025, 0.10273857, 0.09907544, 0.09912481, 0.09630618],\n",
       "       [0.1011029 , 0.10060849, 0.09916362, 0.09927274, 0.10159013,\n",
       "        0.10293733, 0.10075529, 0.09880209, 0.10006677, 0.09570059],\n",
       "       [0.10046076, 0.10050082, 0.09892584, 0.09912561, 0.1017058 ,\n",
       "        0.10437217, 0.10057768, 0.09785392, 0.10133582, 0.09514159],\n",
       "       [0.10013927, 0.1000023 , 0.10018488, 0.10024513, 0.10041463,\n",
       "        0.10105596, 0.10244386, 0.09917473, 0.09884476, 0.09749445],\n",
       "       [0.10055335, 0.10122829, 0.10032797, 0.09876836, 0.10044432,\n",
       "        0.10254117, 0.10236828, 0.09872661, 0.09872429, 0.09631743],\n",
       "       [0.09953683, 0.09986138, 0.10013624, 0.10054504, 0.10020885,\n",
       "        0.10197984, 0.10280575, 0.09778085, 0.09995206, 0.09719317],\n",
       "       [0.09984533, 0.10121643, 0.10021764, 0.09803113, 0.10187691,\n",
       "        0.10280477, 0.10129342, 0.09932254, 0.09910979, 0.09628209],\n",
       "       [0.09968291, 0.10046612, 0.10063446, 0.09878291, 0.10049262,\n",
       "        0.10266542, 0.10204685, 0.0982513 , 0.09911989, 0.0978575 ],\n",
       "       [0.10055738, 0.09952831, 0.0997554 , 0.10025329, 0.10002223,\n",
       "        0.10132112, 0.10243996, 0.09899063, 0.09938429, 0.09774736],\n",
       "       [0.09927735, 0.10048885, 0.10118163, 0.09953097, 0.09941425,\n",
       "        0.1022841 , 0.10242943, 0.09896311, 0.09854778, 0.09788243],\n",
       "       [0.10009107, 0.1010269 , 0.10041296, 0.09826145, 0.10161378,\n",
       "        0.10287141, 0.10105966, 0.09923653, 0.09848518, 0.09694104],\n",
       "       [0.09667262, 0.10126882, 0.10125132, 0.10003842, 0.10144314,\n",
       "        0.10330182, 0.10133911, 0.10021003, 0.0976446 , 0.09683015],\n",
       "       [0.09921861, 0.10045117, 0.09980802, 0.09995646, 0.10037508,\n",
       "        0.10294326, 0.10171556, 0.09907442, 0.09994161, 0.09651577],\n",
       "       [0.09870285, 0.09925562, 0.10017513, 0.10042269, 0.10089073,\n",
       "        0.10293756, 0.10200518, 0.09855588, 0.09892426, 0.09813012],\n",
       "       [0.0997528 , 0.10110702, 0.09931347, 0.09874845, 0.10214123,\n",
       "        0.10379024, 0.10064329, 0.099072  , 0.09972221, 0.09570931],\n",
       "       [0.10048237, 0.10072366, 0.10020288, 0.09794093, 0.10097414,\n",
       "        0.10276094, 0.10102395, 0.09948087, 0.09857284, 0.09783745],\n",
       "       [0.09891618, 0.10066399, 0.10034988, 0.09943738, 0.10139097,\n",
       "        0.10295878, 0.10195026, 0.09864502, 0.09961682, 0.09607071],\n",
       "       [0.0988919 , 0.10064126, 0.10129181, 0.09909265, 0.10071411,\n",
       "        0.10181824, 0.10272256, 0.09842391, 0.09918998, 0.0972136 ],\n",
       "       [0.09928578, 0.101013  , 0.09973352, 0.0989143 , 0.10177084,\n",
       "        0.103568  , 0.10089874, 0.09918689, 0.09992891, 0.0957001 ],\n",
       "       [0.09989209, 0.10126869, 0.10014401, 0.09824178, 0.10174727,\n",
       "        0.10312585, 0.10093444, 0.09885676, 0.09963339, 0.09615567]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 16\n",
    "timesteps = 8\n",
    "num_classes = 10\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32))  # return a single vector of dimension 32\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy training data\n",
    "x_train = np.random.random((1000, timesteps, data_dim))\n",
    "y_train = np.random.random((1000, num_classes))\n",
    "\n",
    "# Generate dummy validation data\n",
    "x_val = np.random.random((100, timesteps, data_dim))\n",
    "y_val = np.random.random((100, num_classes))\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64, epochs=5,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "\n",
    "model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06128578 0.30929189 0.66988104 0.40196385 0.24251233 0.08008038\n",
      " 0.8702245  0.83091881 0.96895065 0.60174026 0.37494061 0.10889286\n",
      " 0.84008072 0.83259979 0.44156355 0.85060293]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
