{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /anaconda3/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /anaconda3/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /anaconda3/lib/python3.6/site-packages (from keras) (1.0.6)\n",
      "Requirement already satisfied: pyyaml in /anaconda3/lib/python3.6/site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: h5py in /anaconda3/lib/python3.6/site-packages (from keras) (2.7.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda3/lib/python3.6/site-packages (from keras) (1.0.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /anaconda3/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /anaconda3/lib/python3.6/site-packages (from keras) (1.14.3)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'p1/head:tx', 'p1/head:ty', 'p1/head:tz', 'p1/hand_l:tx',\n",
      "       'p1/hand_l:ty', 'p1/hand_l:tz', 'p1/hand_r:tx', 'p1/hand_r:ty',\n",
      "       'p1/hand_r:tz', 'p1/foot_l:tx', 'p1/foot_l:ty', 'p1/foot_l:tz',\n",
      "       'p1/foot_r:tx', 'p1/foot_r:ty', 'p1/foot_r:tz', 'p1/hip_l:tx',\n",
      "       'p1/hip_l:ty', 'p1/hip_l:tz', 'p1/hip_r:tx', 'p1/hip_r:ty',\n",
      "       'p1/hip_r:tz', 'p1/shoulder_l:tx', 'p1/shoulder_l:ty',\n",
      "       'p1/shoulder_l:tz', 'p1/shoulder_r:tx', 'p1/shoulder_r:ty',\n",
      "       'p1/shoulder_r:tz', 'p1/shoulder_c:tx', 'p1/shoulder_c:ty',\n",
      "       'p1/shoulder_c:tz', 'p1/knee_l:tx', 'p1/knee_l:ty', 'p1/knee_l:tz',\n",
      "       'p1/knee_r:tx', 'p1/knee_r:ty', 'p1/knee_r:tz', 'p1/elbow_l:tx',\n",
      "       'p1/elbow_l:ty', 'p1/elbow_l:tz', 'p1/elbow_r:tx', 'p1/elbow_r:ty',\n",
      "       'p1/elbow_r:tz', 'p1/head:dx', 'p1/head:dy', 'p1/head:dz',\n",
      "       'p1/hand_l:dx', 'p1/hand_l:dy', 'p1/hand_l:dz', 'p1/hand_r:dx',\n",
      "       'p1/hand_r:dy', 'p1/hand_r:dz', 'p1/foot_l:dx', 'p1/foot_l:dy',\n",
      "       'p1/foot_l:dz', 'p1/foot_r:dx', 'p1/foot_r:dy', 'p1/foot_r:dz',\n",
      "       'p1/hip_l:dx', 'p1/hip_l:dy', 'p1/hip_l:dz', 'p1/hip_r:dx',\n",
      "       'p1/hip_r:dy', 'p1/hip_r:dz', 'p1/shoulder_l:dx', 'p1/shoulder_l:dy',\n",
      "       'p1/shoulder_l:dz', 'p1/shoulder_r:dx', 'p1/shoulder_r:dy',\n",
      "       'p1/shoulder_r:dz', 'p1/shoulder_c:dx', 'p1/shoulder_c:dy',\n",
      "       'p1/shoulder_c:dz', 'p1/knee_l:dx', 'p1/knee_l:dy', 'p1/knee_l:dz',\n",
      "       'p1/knee_r:dx', 'p1/knee_r:dy', 'p1/knee_r:dz', 'p1/elbow_l:dx',\n",
      "       'p1/elbow_l:dy', 'p1/elbow_l:dz', 'p1/elbow_r:dx', 'p1/elbow_r:dy',\n",
      "       'p1/elbow_r:dz', 'classification', 'user'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"cleaned_data.csv\", sep=\",\")\n",
    "\n",
    "columns = data.columns\n",
    "\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dancing_alden.tsv\n",
      "dancing_asha.tsv\n",
      "dancing_dan.tsv\n",
      "dancing_jason.tsv\n",
      "dancing_mai.tsv\n",
      "dancing_max.tsv\n",
      "dancing_me.tsv\n",
      "dancing_nico.tsv\n",
      "dancing_nicolas.tsv\n",
      "dancing_roland.tsv\n",
      "dancing_will.tsv\n",
      "notdancing_alden.tsv\n",
      "notdancing_asha.tsv\n",
      "notdancing_dan.tsv\n",
      "notdancing_jason.tsv\n",
      "notdancing_mai.tsv\n",
      "notdancing_max.tsv\n",
      "notdancing_me.tsv\n",
      "notdancing_nico.tsv\n",
      "notdancing_nicolas.tsv\n",
      "notdancing_roland.tsv\n",
      "notdancing_shreiya.tsv\n",
      "notdancing_will.tsv\n",
      "18\n",
      "5\n",
      "0.0\n",
      "[[ 2.4009160e-02  4.0241600e-03  2.6284400e-02 ...  2.3928320e-02\n",
      "   7.6115480e-03  1.6121600e-02]\n",
      " [ 4.3428700e-02  1.1206000e-03  1.8739200e-02 ...  9.4422600e-03\n",
      "   6.0066714e-02 -1.1294600e-02]\n",
      " [ 1.6822360e-02  1.3038920e-02  3.4292000e-03 ...  1.2720400e-03\n",
      "   5.5485612e-02 -1.4593200e-02]\n",
      " ...\n",
      " [ 4.8120000e-04 -6.5086000e-04 -2.4240000e-04 ... -3.1134000e-04\n",
      "  -1.5761200e-04  7.7780000e-04]\n",
      " [ 8.5248000e-04 -1.2180000e-03 -6.6040000e-04 ...  5.2044000e-04\n",
      "   6.8520000e-06  1.0500000e-03]\n",
      " [ 2.1564400e-03 -3.2135800e-03 -1.9414000e-03 ...  1.6140600e-03\n",
      "  -6.2612800e-04  3.6400000e-03]]\n"
     ]
    }
   ],
   "source": [
    "timesteps = 200\n",
    "input_dim = 42\n",
    "\n",
    "prev_user = None\n",
    "train_data = []\n",
    "xs = []\n",
    "\n",
    "for row in data.itertuples():\n",
    "    user = row.user\n",
    "    classification = row.classification\n",
    "    \n",
    "    if user != prev_user:\n",
    "        if prev_user == None:\n",
    "            prev_user = user\n",
    "            next\n",
    "        \n",
    "        print(user)\n",
    "        prev_user = user\n",
    "        train_data.append([xs, classification])\n",
    "        xs = []\n",
    "\n",
    "\n",
    "    x = row[-44:-2]\n",
    "    xs.extend([x])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "\n",
    "# print(train_data[3])\n",
    "\n",
    "dftrain, dftest = train_test_split(train_data, test_size=0.2)\n",
    "x_train = [np.array(col[0]) for col in dftrain]\n",
    "y_train = [col[1] for col in dftrain]\n",
    "x_test = [np.array(col[0]) for col in dftest]\n",
    "y_test = [col[1] for col in dftest]\n",
    "\n",
    "\n",
    "print(len(dftrain))\n",
    "print(len(dftest))\n",
    "\n",
    "print(y_train[0])\n",
    "print(x_train[0])\n",
    "\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      p1/head:dx  p1/head:dy  p1/head:dz  p1/hand_l:dx  p1/hand_l:dy  \\\n",
      "703     0.003441   -0.010583   -0.028262      0.051805     -0.006752   \n",
      "1783   -0.022781   -0.002775    0.008987     -0.014053     -0.005014   \n",
      "1370    0.023194    0.008591   -0.001308      0.034563      0.039009   \n",
      "3431    0.028514    0.000297    0.002326      0.025015      0.005417   \n",
      "1205    0.071625   -0.015186   -0.034286      0.037128     -0.000814   \n",
      "433    -0.002718   -0.003186   -0.008066     -0.004913     -0.027240   \n",
      "797     0.010013   -0.017901   -0.016801      0.026054     -0.078083   \n",
      "3118    0.011165    0.004047   -0.003732     -0.037443      0.028383   \n",
      "4231    0.029788   -0.008503   -0.006106      0.010882     -0.000063   \n",
      "2253   -0.004144   -0.003619    0.000968     -0.020249      0.025333   \n",
      "1875   -0.028627   -0.025452    0.058227      0.133389     -0.198524   \n",
      "76      0.015046   -0.007204    0.052710     -0.110130     -0.034202   \n",
      "2617   -0.016449    0.000508    0.016336     -0.029788     -0.016252   \n",
      "2108    0.003897    0.005831   -0.000231      0.000019      0.007680   \n",
      "3149   -0.009812    0.009518    0.019505     -0.032096     -0.006328   \n",
      "1367    0.001327   -0.007258    0.022290     -0.009290     -0.029815   \n",
      "913     0.018752   -0.013339    0.016599      0.039957     -0.003619   \n",
      "2130   -0.042673   -0.002880   -0.008789     -0.014149     -0.004095   \n",
      "1947    0.006406   -0.055115    0.026829      0.112218     -0.120534   \n",
      "1968   -0.023450    0.009976    0.007392      0.234209     -0.233241   \n",
      "1180    0.072460   -0.043102   -0.015231      0.033290     -0.112571   \n",
      "2932   -0.005082   -0.001943   -0.003631     -0.010875      0.010619   \n",
      "2614   -0.037335   -0.000211    0.031241     -0.029437     -0.003540   \n",
      "3336   -0.070312    0.004647    0.018498     -0.056314      0.001913   \n",
      "1479   -0.035052    0.015563   -0.023022     -0.002627      0.032548   \n",
      "1866    0.003988   -0.022985    0.013779      0.168312      0.054529   \n",
      "2488    0.001793   -0.012530   -0.019181      0.000909     -0.001292   \n",
      "2257   -0.002898    0.002668    0.002014      0.010802     -0.030309   \n",
      "3259   -0.017404    0.007159    0.028392     -0.051832      0.017012   \n",
      "835    -0.008142   -0.014193    0.003100      0.018606      0.005674   \n",
      "...          ...         ...         ...           ...           ...   \n",
      "512     0.011223   -0.007039   -0.002997     -0.026519     -0.010664   \n",
      "753     0.017675   -0.018380   -0.014633      0.062133      0.008937   \n",
      "4230    0.035706   -0.001323   -0.007590      0.026107      0.001348   \n",
      "1614    0.000391    0.025758    0.036561      0.114497      0.043585   \n",
      "220    -0.002578    0.012552    0.053423      0.072475      0.094401   \n",
      "3885   -0.006328   -0.068508   -0.113561      0.051198     -0.133249   \n",
      "2822    0.000679   -0.000595    0.001168      0.005110     -0.020193   \n",
      "200     0.014461    0.016596    0.011116     -0.007004     -0.010542   \n",
      "1105   -0.008431    0.057664    0.048089     -0.245580      0.032738   \n",
      "2307   -0.011716   -0.005619   -0.005325     -0.058383      0.012847   \n",
      "359    -0.024515    0.032924   -0.006165      0.008878      0.017444   \n",
      "1784   -0.011883   -0.022981    0.040376     -0.008645      0.003715   \n",
      "1852    0.044162    0.044953   -0.000311      0.213735     -0.080066   \n",
      "2690    0.036506   -0.009072    0.000455      0.047039     -0.002362   \n",
      "4027   -0.006238   -0.024561   -0.045087     -0.021965      0.019731   \n",
      "880    -0.011709   -0.002300    0.004256     -0.090681      0.043778   \n",
      "204     0.055353   -0.017132    0.023038      0.026572     -0.027110   \n",
      "976    -0.019659    0.003652    0.023294     -0.014681      0.004150   \n",
      "448    -0.013833   -0.012658   -0.037089     -0.014789     -0.044462   \n",
      "1050    0.018244    0.014500    0.034728     -0.005799     -0.101500   \n",
      "379    -0.014716   -0.028341   -0.012215     -0.014317     -0.064130   \n",
      "4000    0.032248   -0.029240   -0.007422      0.030314      0.311202   \n",
      "2545    0.043286   -0.009146   -0.009655      0.060951     -0.006276   \n",
      "3196    0.029977    0.014633   -0.035616      0.037122     -0.009480   \n",
      "4222    0.023483   -0.010434   -0.012934      0.002597     -0.006931   \n",
      "3314   -0.029807    0.004435    0.014290     -0.015387      0.024144   \n",
      "860    -0.006655    0.020617    0.000189     -0.049365     -0.031827   \n",
      "1046   -0.005042    0.027690    0.061424      0.001662      0.030941   \n",
      "4010   -0.010703    0.023368    0.028469      0.030482     -0.088336   \n",
      "3316   -0.031932    0.008537    0.009885     -0.016062      0.005231   \n",
      "\n",
      "      p1/hand_l:dz  p1/hand_r:dx  p1/hand_r:dy  p1/hand_r:dz  p1/foot_l:dx  \\\n",
      "703      -0.034187     -0.052529      0.019421      0.053922     -0.009796   \n",
      "1783      0.013518     -0.013307     -0.003674      0.018317      0.007546   \n",
      "1370     -0.015059      0.025400      0.070303     -0.030486     -0.002064   \n",
      "3431      0.008658      0.021553     -0.000335      0.005732     -0.000388   \n",
      "1205      0.004896      0.041628     -0.019050     -0.020399      0.084839   \n",
      "433      -0.039105      0.042734      0.067701      0.093468      0.006905   \n",
      "797      -0.059408     -0.000373     -0.047542      0.011727     -0.040000   \n",
      "3118      0.113089     -0.010339     -0.016996      0.012317     -0.007444   \n",
      "4231     -0.002438      0.043527     -0.020928      0.015317      0.002877   \n",
      "2253     -0.020131      0.007622     -0.002207     -0.002192      0.004756   \n",
      "1875      0.091360      0.110181     -0.087246      0.124438      0.128435   \n",
      "76        0.096616      0.015071     -0.180807      0.008288      0.122584   \n",
      "2617      0.010590     -0.024926      0.001684      0.017958     -0.107764   \n",
      "2108     -0.006551      0.006910      0.007386      0.003049      0.001162   \n",
      "3149      0.032628      0.124126     -0.069180      0.200226     -0.018288   \n",
      "1367      0.007788     -0.000813     -0.075225      0.036367     -0.008982   \n",
      "913      -0.006331     -0.013083     -0.014645     -0.008445     -0.021516   \n",
      "2130      0.052826     -0.003636     -0.001783     -0.006257     -0.023065   \n",
      "1947      0.040900      0.093083     -0.151312      0.057731      0.174271   \n",
      "1968     -0.026988      0.104944      0.012888      0.078317      0.009467   \n",
      "1180      0.015378     -0.104370      0.022367     -0.013972      0.029355   \n",
      "2932      0.012654     -0.119400      0.160799      0.049479      0.003507   \n",
      "2614      0.012707     -0.034095     -0.000255      0.006535      0.048630   \n",
      "3336      0.017625     -0.071666     -0.003524      0.023186     -0.042184   \n",
      "1479     -0.010048     -0.006367      0.040608     -0.026881     -0.049101   \n",
      "1866     -0.159630      0.222766      0.074865     -0.024131     -0.028177   \n",
      "2488     -0.009422      0.005017     -0.001456     -0.010019      0.026419   \n",
      "2257      0.024543     -0.000125     -0.001788      0.000594     -0.001118   \n",
      "3259      0.003411      0.071515      0.053238      0.244131      0.057767   \n",
      "835       0.025949      0.229557     -0.107475     -0.123116      0.034505   \n",
      "...            ...           ...           ...           ...           ...   \n",
      "512      -0.088120     -0.005148     -0.084378      0.005513     -0.023038   \n",
      "753      -0.039582      0.040187      0.076318      0.021454     -0.009875   \n",
      "4230     -0.006054      0.057367     -0.019701      0.018076      0.037878   \n",
      "1614     -0.022967      0.133917      0.026136      0.128000      0.146762   \n",
      "220       0.076054      0.042876      0.035212      0.161741     -0.016420   \n",
      "3885     -0.028257      0.041596     -0.101412     -0.002681     -0.127656   \n",
      "2822     -0.109843     -0.002354     -0.002411     -0.007036     -0.006006   \n",
      "200       0.006860     -0.020437      0.062755     -0.033867     -0.085534   \n",
      "1105     -0.216697     -0.019917      0.119007     -0.014507     -0.014262   \n",
      "2307      0.040622     -0.011961     -0.001523     -0.006345      0.002504   \n",
      "359       0.024540      0.028064      0.158449      0.083055      0.024476   \n",
      "1784      0.007272     -0.000962     -0.001291      0.008188     -0.007648   \n",
      "1852      0.092485      0.079802      0.114431     -0.019055      0.085731   \n",
      "2690      0.004909      0.043912     -0.017143      0.012836      0.000844   \n",
      "4027      0.020300      0.080607     -0.042135      0.012567     -0.048331   \n",
      "880       0.092086     -0.030342     -0.085443     -0.035588     -0.038506   \n",
      "204       0.042642      0.086975     -0.155743      0.025711      0.006468   \n",
      "976       0.035740     -0.037873      0.010315     -0.031340     -0.094458   \n",
      "448      -0.052040     -0.035033      0.004353      0.004163     -0.005721   \n",
      "1050     -0.116892      0.016035     -0.000686      0.103048     -0.005950   \n",
      "379      -0.007008     -0.080387     -0.061871     -0.085051     -0.018318   \n",
      "4000     -0.037554      0.054732      0.036742     -0.154295      0.005268   \n",
      "2545     -0.013512     -0.016728     -0.071859     -0.113354     -0.008352   \n",
      "3196     -0.023875      0.025588     -0.006754     -0.026079      0.024479   \n",
      "4222     -0.001553      0.013098     -0.012647     -0.019153     -0.001596   \n",
      "3314      0.003158     -0.049634      0.014054      0.003140      0.024425   \n",
      "860      -0.002658     -0.087286      0.009656     -0.042905      0.002845   \n",
      "1046      0.070187     -0.208369      0.146608      0.003720      0.034124   \n",
      "4010     -0.046194      0.009412     -0.007736     -0.044620      0.011437   \n",
      "3316     -0.033742     -0.047994     -0.004904      0.006380     -0.156026   \n",
      "\n",
      "           ...        p1/knee_r:dx  p1/knee_r:dy  p1/knee_r:dz  p1/elbow_l:dx  \\\n",
      "703        ...            0.006884     -0.013237     -0.030061       0.034842   \n",
      "1783       ...            0.005451     -0.000555     -0.016004       0.000011   \n",
      "1370       ...            0.005112      0.028011     -0.006400       0.022982   \n",
      "3431       ...            0.003900     -0.000503      0.003348       0.026073   \n",
      "1205       ...            0.009510      0.079438     -0.107370       0.050038   \n",
      "433        ...           -0.027452      0.022267     -0.027342      -0.018016   \n",
      "797        ...            0.009235      0.006514     -0.022276       0.032001   \n",
      "3118       ...           -0.019978     -0.003810      0.008544       0.012689   \n",
      "4231       ...            0.058284      0.016532     -0.026123       0.014888   \n",
      "2253       ...           -0.001221     -0.002134      0.000596      -0.001420   \n",
      "1875       ...           -0.021015     -0.010111      0.022065       0.009752   \n",
      "76         ...           -0.101811      0.082515      0.163561      -0.166802   \n",
      "2617       ...           -0.024549      0.006270      0.008840      -0.026090   \n",
      "2108       ...            0.012038      0.011263     -0.000123       0.000923   \n",
      "3149       ...           -0.011874      0.007156      0.032906      -0.034063   \n",
      "1367       ...           -0.037183     -0.027213     -0.010578       0.008630   \n",
      "913        ...            0.031480      0.001602      0.005936       0.044189   \n",
      "2130       ...            0.036223      0.012496     -0.043285      -0.027141   \n",
      "1947       ...            0.039552     -0.070688     -0.003690       0.083867   \n",
      "1968       ...           -0.008227      0.062042      0.007916       0.000122   \n",
      "1180       ...           -0.013290     -0.044807      0.007139       0.088022   \n",
      "2932       ...           -0.015604     -0.017879      0.014525      -0.006749   \n",
      "2614       ...           -0.015766     -0.021112      0.009884      -0.024469   \n",
      "3336       ...           -0.025080      0.015006      0.007729      -0.051651   \n",
      "1479       ...           -0.053066      0.037743      0.075856      -0.017880   \n",
      "1866       ...           -0.035366      0.004150     -0.010942       0.169418   \n",
      "2488       ...            0.005359      0.011458     -0.019488      -0.003734   \n",
      "2257       ...           -0.000242     -0.000222      0.000390       0.011285   \n",
      "3259       ...           -0.005618      0.012528     -0.020488      -0.010732   \n",
      "835        ...            0.084414     -0.057755      0.095229       0.019245   \n",
      "...        ...                 ...           ...           ...            ...   \n",
      "512        ...            0.004668      0.011959     -0.079647      -0.003957   \n",
      "753        ...            0.020216     -0.011984      0.022478      -0.008669   \n",
      "4230       ...            0.030401     -0.005888     -0.003204       0.018595   \n",
      "1614       ...           -0.040488      0.008173      0.031471       0.043145   \n",
      "220        ...            0.003630      0.013419      0.054596       0.054799   \n",
      "3885       ...            0.048917     -0.041154      0.010071      -0.183727   \n",
      "2822       ...            0.002330     -0.003909      0.001400       0.003873   \n",
      "200        ...           -0.023780      0.034768      0.045156       0.003819   \n",
      "1105       ...            0.020816      0.129747     -0.036106       0.070642   \n",
      "2307       ...           -0.010053     -0.002964     -0.076421      -0.032250   \n",
      "359        ...           -0.001463      0.005630     -0.007806       0.007415   \n",
      "1784       ...           -0.000692      0.006304     -0.004002      -0.013710   \n",
      "1852       ...            0.061759      0.023254      0.112706       0.099678   \n",
      "2690       ...            0.006309     -0.004093      0.010292       0.016489   \n",
      "4027       ...            0.015036     -0.078447     -0.058169       0.008214   \n",
      "880        ...           -0.008928     -0.004203     -0.007950      -0.015025   \n",
      "204        ...            0.014694     -0.016102      0.012186       0.020947   \n",
      "976        ...           -0.009279      0.009465      0.008014      -0.017915   \n",
      "448        ...           -0.026041      0.022370     -0.007289      -0.046712   \n",
      "1050       ...            0.024222     -0.027790      0.006542       0.033874   \n",
      "379        ...            0.004513     -0.043202     -0.027994       0.013640   \n",
      "4000       ...            0.023205     -0.041294     -0.013395       0.018119   \n",
      "2545       ...            0.027470      0.004101     -0.006895       0.052318   \n",
      "3196       ...            0.046602     -0.012169      0.002107       0.020202   \n",
      "4222       ...           -0.009374     -0.014896     -0.012728       0.012044   \n",
      "3314       ...           -0.034646      0.004707      0.007344      -0.006725   \n",
      "860        ...            0.004357     -0.006905      0.029609      -0.009300   \n",
      "1046       ...           -0.016533      0.025807      0.053535      -0.009274   \n",
      "4010       ...            0.036029     -0.012616      0.016216      -0.017578   \n",
      "3316       ...           -0.027051      0.010624      0.010965      -0.016138   \n",
      "\n",
      "      p1/elbow_l:dy  p1/elbow_l:dz  p1/elbow_r:dx  p1/elbow_r:dy  \\\n",
      "703       -0.013948      -0.029150      -0.016795      -0.030825   \n",
      "1783      -0.009511       0.017945      -0.005350       0.001224   \n",
      "1370       0.017098      -0.004036       0.014996       0.042152   \n",
      "3431       0.007112       0.010638       0.010182      -0.004309   \n",
      "1205       0.005840      -0.010188       0.029958       0.009105   \n",
      "433       -0.018557      -0.025786       0.046901      -0.002265   \n",
      "797       -0.050060      -0.004774       0.014640      -0.010576   \n",
      "3118       0.006191      -0.012135       0.000847      -0.003182   \n",
      "4231      -0.006027       0.005428       0.032333      -0.004401   \n",
      "2253       0.005024      -0.001322      -0.001431      -0.010792   \n",
      "1875      -0.007487       0.121390       0.037119      -0.025324   \n",
      "76         0.042120       0.334464       0.055832       0.031373   \n",
      "2617      -0.012281       0.021242      -0.024491       0.000426   \n",
      "2108      -0.000065      -0.002536      -0.025029      -0.004328   \n",
      "3149       0.001573       0.037041       0.120946      -0.035483   \n",
      "1367      -0.004028       0.004637       0.006073      -0.043907   \n",
      "913       -0.007163      -0.009586       0.010375      -0.020284   \n",
      "2130      -0.001308       0.021140      -0.000824      -0.002332   \n",
      "1947      -0.106784       0.077550       0.068998      -0.094370   \n",
      "1968       0.041953       0.007027       0.014092       0.032770   \n",
      "1180      -0.037424      -0.039144      -0.047205      -0.028037   \n",
      "2932       0.005189       0.012980      -0.104570       0.002729   \n",
      "2614       0.003085       0.017812      -0.043556       0.004895   \n",
      "3336       0.009085       0.012302      -0.069358       0.003692   \n",
      "1479       0.009590      -0.007579      -0.007278       0.032586   \n",
      "1866       0.054621      -0.112833      -0.016084       0.001407   \n",
      "2488      -0.011900      -0.014733       0.005994       0.001988   \n",
      "2257      -0.027765       0.013168      -0.000800      -0.000233   \n",
      "3259       0.016367       0.037855      -0.014847      -0.000635   \n",
      "835        0.008673       0.009958       0.122890      -0.080818   \n",
      "...             ...            ...            ...            ...   \n",
      "512       -0.005581       0.000850       0.001640      -0.044209   \n",
      "753       -0.011688      -0.071190       0.030023      -0.001679   \n",
      "4230      -0.002017       0.000697       0.034725       0.002466   \n",
      "1614       0.027503       0.113008       0.059595       0.082819   \n",
      "220        0.101599       0.059705      -0.005430       0.001418   \n",
      "3885       0.032211       0.060586       0.024001      -0.073346   \n",
      "2822      -0.023688      -0.037489       0.000995       0.001315   \n",
      "200        0.003503      -0.010199      -0.029674       0.043602   \n",
      "1105       0.040590       0.011930       0.004005       0.047427   \n",
      "2307       0.001429      -0.010746      -0.012275      -0.000858   \n",
      "359        0.026698      -0.000428       0.010245       0.114982   \n",
      "1784       0.014478       0.008903      -0.000083      -0.003176   \n",
      "1852      -0.040263       0.015625       0.107804       0.086891   \n",
      "2690       0.014160       0.007017       0.029433      -0.015787   \n",
      "4027      -0.027188      -0.012502       0.029819      -0.006062   \n",
      "880       -0.006019       0.024197      -0.023206       0.003322   \n",
      "204       -0.048015      -0.005487       0.016816       0.005228   \n",
      "976        0.006851       0.037056       0.002650       0.026119   \n",
      "448       -0.028386      -0.084914      -0.047424       0.006067   \n",
      "1050      -0.087995      -0.021364      -0.055927       0.036675   \n",
      "379       -0.005490       0.013726      -0.023265      -0.061348   \n",
      "4000      -0.067150       0.010543       0.022633      -0.004992   \n",
      "2545      -0.010080      -0.025046       0.017428      -0.017605   \n",
      "3196      -0.007730      -0.018616       0.025655      -0.006788   \n",
      "4222      -0.004028      -0.008395       0.013212      -0.012732   \n",
      "3314       0.005137       0.014645      -0.029424       0.011635   \n",
      "860        0.003109      -0.010221      -0.035533       0.008663   \n",
      "1046       0.002177       0.064440      -0.022968      -0.013719   \n",
      "4010      -0.057257       0.039432       0.012827       0.094336   \n",
      "3316       0.006381      -0.004918      -0.047738       0.002002   \n",
      "\n",
      "      p1/elbow_r:dz  classification  \n",
      "703        0.015461             1.0  \n",
      "1783       0.009317             1.0  \n",
      "1370      -0.018897             1.0  \n",
      "3431       0.008505             0.0  \n",
      "1205      -0.006404             1.0  \n",
      "433        0.074869             1.0  \n",
      "797        0.017416             1.0  \n",
      "3118       0.000770             0.0  \n",
      "4231      -0.005278             0.0  \n",
      "2253       0.009042             0.0  \n",
      "1875       0.137824             1.0  \n",
      "76         0.091552             1.0  \n",
      "2617       0.012579             0.0  \n",
      "2108      -0.029915             0.0  \n",
      "3149       0.199344             0.0  \n",
      "1367       0.030460             1.0  \n",
      "913       -0.034798             1.0  \n",
      "2130       0.001597             0.0  \n",
      "1947       0.021238             1.0  \n",
      "1968       0.027537             1.0  \n",
      "1180      -0.017974             1.0  \n",
      "2932      -0.010859             0.0  \n",
      "2614       0.021100             0.0  \n",
      "3336       0.013039             0.0  \n",
      "1479      -0.051278             1.0  \n",
      "1866       0.009152             1.0  \n",
      "2488      -0.014058             0.0  \n",
      "2257       0.000178             0.0  \n",
      "3259       0.014021             0.0  \n",
      "835       -0.053103             1.0  \n",
      "...             ...             ...  \n",
      "512       -0.036189             1.0  \n",
      "753        0.018810             1.0  \n",
      "4230      -0.008665             0.0  \n",
      "1614       0.027921             1.0  \n",
      "220        0.072103             1.0  \n",
      "3885      -0.149848             0.0  \n",
      "2822       0.002366             0.0  \n",
      "200        0.008012             1.0  \n",
      "1105      -0.013054             1.0  \n",
      "2307      -0.019367             0.0  \n",
      "359       -0.043342             1.0  \n",
      "1784       0.005430             1.0  \n",
      "1852      -0.017585             1.0  \n",
      "2690       0.022516             0.0  \n",
      "4027       0.047362             0.0  \n",
      "880       -0.043268             1.0  \n",
      "204        0.029713             1.0  \n",
      "976        0.013233             1.0  \n",
      "448       -0.021877             1.0  \n",
      "1050       0.034953             1.0  \n",
      "379       -0.064098             1.0  \n",
      "4000       0.027182             0.0  \n",
      "2545      -0.013705             0.0  \n",
      "3196      -0.026114             0.0  \n",
      "4222      -0.019168             0.0  \n",
      "3314       0.000058             0.0  \n",
      "860       -0.021420             1.0  \n",
      "1046       0.054573             1.0  \n",
      "4010      -0.019142             0.0  \n",
      "3316       0.005763             0.0  \n",
      "\n",
      "[3385 rows x 43 columns]\n",
      "703     1.0\n",
      "1783    1.0\n",
      "1370    1.0\n",
      "3431    0.0\n",
      "1205    1.0\n",
      "433     1.0\n",
      "797     1.0\n",
      "3118    0.0\n",
      "4231    0.0\n",
      "2253    0.0\n",
      "1875    1.0\n",
      "76      1.0\n",
      "2617    0.0\n",
      "2108    0.0\n",
      "3149    0.0\n",
      "1367    1.0\n",
      "913     1.0\n",
      "2130    0.0\n",
      "1947    1.0\n",
      "1968    1.0\n",
      "1180    1.0\n",
      "2932    0.0\n",
      "2614    0.0\n",
      "3336    0.0\n",
      "1479    1.0\n",
      "1866    1.0\n",
      "2488    0.0\n",
      "2257    0.0\n",
      "3259    0.0\n",
      "835     1.0\n",
      "       ... \n",
      "512     1.0\n",
      "753     1.0\n",
      "4230    0.0\n",
      "1614    1.0\n",
      "220     1.0\n",
      "3885    0.0\n",
      "2822    0.0\n",
      "200     1.0\n",
      "1105    1.0\n",
      "2307    0.0\n",
      "359     1.0\n",
      "1784    1.0\n",
      "1852    1.0\n",
      "2690    0.0\n",
      "4027    0.0\n",
      "880     1.0\n",
      "204     1.0\n",
      "976     1.0\n",
      "448     1.0\n",
      "1050    1.0\n",
      "379     1.0\n",
      "4000    0.0\n",
      "2545    0.0\n",
      "3196    0.0\n",
      "4222    0.0\n",
      "3314    0.0\n",
      "860     1.0\n",
      "1046    1.0\n",
      "4010    0.0\n",
      "3316    0.0\n",
      "Name: classification, Length: 3385, dtype: float64\n",
      "['p1/head:dx', 'p1/head:dy', 'p1/head:dz', 'p1/hand_l:dx', 'p1/hand_l:dy', 'p1/hand_l:dz', 'p1/hand_r:dx', 'p1/hand_r:dy', 'p1/hand_r:dz', 'p1/foot_l:dx', 'p1/foot_l:dy', 'p1/foot_l:dz', 'p1/foot_r:dx', 'p1/foot_r:dy', 'p1/foot_r:dz', 'p1/hip_l:dx', 'p1/hip_l:dy', 'p1/hip_l:dz', 'p1/hip_r:dx', 'p1/hip_r:dy', 'p1/hip_r:dz', 'p1/shoulder_l:dx', 'p1/shoulder_l:dy', 'p1/shoulder_l:dz', 'p1/shoulder_r:dx', 'p1/shoulder_r:dy', 'p1/shoulder_r:dz', 'p1/shoulder_c:dx', 'p1/shoulder_c:dy', 'p1/shoulder_c:dz', 'p1/knee_l:dx', 'p1/knee_l:dy', 'p1/knee_l:dz', 'p1/knee_r:dx', 'p1/knee_r:dy', 'p1/knee_r:dz', 'p1/elbow_l:dx', 'p1/elbow_l:dy', 'p1/elbow_l:dz', 'p1/elbow_r:dx', 'p1/elbow_r:dy', 'p1/elbow_r:dz', 'classification']\n"
     ]
    }
   ],
   "source": [
    "diff_columns = [col for col in columns if re.match(r\".*d.$\", col)]\n",
    "diff_columns.extend(['classification'])\n",
    "\n",
    "dftrain = data[diff_columns]\n",
    "\n",
    "dftrain, dftest = train_test_split(data[diff_columns], test_size=0.2)\n",
    "\n",
    "# Training data\n",
    "x_train = dftrain[diff_columns]\n",
    "y_train = dftrain['classification']\n",
    "\n",
    "# Test data\n",
    "x_test = dftest[diff_columns]\n",
    "y_test = dftest['classification']\n",
    "\n",
    "print(x_train)\n",
    "print(y_train)\n",
    "\n",
    "print(diff_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "42\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-b3e1de1a46a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfakex_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfakex_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# from example\n",
    "\n",
    "fakex_train = np.random.random((1000, timesteps, data_dim))\n",
    "fakey_train = np.random.random((1000, num_classes))\n",
    "\n",
    "print(timesteps)\n",
    "print(data_dim)\n",
    "print(type(fakex_train))\n",
    "print(type(np.ndarray(x_train)))\n",
    "print(fakex_train[2])\n",
    "print(x_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 18 arrays: [array([[ 0.0234914 ,  0.0036742 ,  0.0066016 , ...,  0.010187  ,\n         0.0057344 , -0.0140008 ],\n       [ 0.02832682,  0.00335702,  0.0022358 , ...,  0.02072374,\n         0.00444246, -0.0116132 ],...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-761962d690ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m model.fit(x_train, y_train,\n\u001b[0;32m---> 19\u001b[0;31m           batch_size=1, epochs=5)\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 18 arrays: [array([[ 0.0234914 ,  0.0036742 ,  0.0066016 , ...,  0.010187  ,\n         0.0057344 , -0.0140008 ],\n       [ 0.02832682,  0.00335702,  0.0022358 , ...,  0.02072374,\n         0.00444246, -0.0116132 ],..."
     ]
    }
   ],
   "source": [
    "data_dim = 42\n",
    "timesteps = 200\n",
    "num_classes = 2\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 42\n",
    "model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32))  # return a single vector of dimension 32\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=1, epochs=5)\n",
    "\n",
    "\n",
    "# model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.34158772 0.76788325 0.55376324 ... 0.73262943 0.79545617 0.43927229]\n",
      " [0.07274669 0.31278729 0.01525534 ... 0.66751965 0.43657974 0.46063277]\n",
      " [0.73012253 0.044247   0.81713062 ... 0.2552919  0.54264535 0.33944658]\n",
      " ...\n",
      " [0.36313183 0.17641259 0.47437952 ... 0.45834552 0.02398759 0.41426352]\n",
      " [0.15092495 0.18552565 0.136178   ... 0.52719038 0.6250653  0.55409114]\n",
      " [0.45209888 0.51370893 0.26052288 ... 0.06023967 0.30422309 0.20964024]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>50</th>\n",
       "      <th>1.3046719999999998</th>\n",
       "      <th>-0.38843049999999996</th>\n",
       "      <th>3.393734</th>\n",
       "      <th>0.8746868</th>\n",
       "      <th>-0.6795787</th>\n",
       "      <th>3.3568839999999995</th>\n",
       "      <th>0.8905584999999999</th>\n",
       "      <th>-0.6281642</th>\n",
       "      <th>3.383114</th>\n",
       "      <th>...</th>\n",
       "      <th>-1.462777</th>\n",
       "      <th>3.551411</th>\n",
       "      <th>1.055417</th>\n",
       "      <th>-0.7448916999999999</th>\n",
       "      <th>3.120533</th>\n",
       "      <th>1.0443719999999999</th>\n",
       "      <th>-0.6931599</th>\n",
       "      <th>3.1474599999999997</th>\n",
       "      <th>alden</th>\n",
       "      <th>dancing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22994</th>\n",
       "      <td>1045</td>\n",
       "      <td>0.805225</td>\n",
       "      <td>-0.064882</td>\n",
       "      <td>2.703505</td>\n",
       "      <td>0.529117</td>\n",
       "      <td>-0.748536</td>\n",
       "      <td>2.817768</td>\n",
       "      <td>0.766553</td>\n",
       "      <td>-0.729576</td>\n",
       "      <td>2.835117</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.123607</td>\n",
       "      <td>3.249849</td>\n",
       "      <td>0.427156</td>\n",
       "      <td>-0.474788</td>\n",
       "      <td>2.765285</td>\n",
       "      <td>0.902814</td>\n",
       "      <td>-0.500578</td>\n",
       "      <td>2.897322</td>\n",
       "      <td>will</td>\n",
       "      <td>not dancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22995</th>\n",
       "      <td>1046</td>\n",
       "      <td>0.807749</td>\n",
       "      <td>-0.072318</td>\n",
       "      <td>2.692815</td>\n",
       "      <td>0.536634</td>\n",
       "      <td>-0.751952</td>\n",
       "      <td>2.808611</td>\n",
       "      <td>0.769255</td>\n",
       "      <td>-0.738360</td>\n",
       "      <td>2.822912</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.132008</td>\n",
       "      <td>3.242096</td>\n",
       "      <td>0.423794</td>\n",
       "      <td>-0.483205</td>\n",
       "      <td>2.754180</td>\n",
       "      <td>0.904506</td>\n",
       "      <td>-0.510160</td>\n",
       "      <td>2.882418</td>\n",
       "      <td>will</td>\n",
       "      <td>not dancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22996</th>\n",
       "      <td>1047</td>\n",
       "      <td>0.807749</td>\n",
       "      <td>-0.072318</td>\n",
       "      <td>2.692815</td>\n",
       "      <td>0.536634</td>\n",
       "      <td>-0.751952</td>\n",
       "      <td>2.808611</td>\n",
       "      <td>0.769255</td>\n",
       "      <td>-0.738360</td>\n",
       "      <td>2.822912</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.132008</td>\n",
       "      <td>3.242096</td>\n",
       "      <td>0.423794</td>\n",
       "      <td>-0.483205</td>\n",
       "      <td>2.754180</td>\n",
       "      <td>0.904506</td>\n",
       "      <td>-0.510160</td>\n",
       "      <td>2.882418</td>\n",
       "      <td>will</td>\n",
       "      <td>not dancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22997</th>\n",
       "      <td>1048</td>\n",
       "      <td>0.808892</td>\n",
       "      <td>-0.081096</td>\n",
       "      <td>2.680727</td>\n",
       "      <td>0.543384</td>\n",
       "      <td>-0.763010</td>\n",
       "      <td>2.796647</td>\n",
       "      <td>0.771182</td>\n",
       "      <td>-0.748278</td>\n",
       "      <td>2.809686</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.137862</td>\n",
       "      <td>3.231271</td>\n",
       "      <td>0.422788</td>\n",
       "      <td>-0.496354</td>\n",
       "      <td>2.734586</td>\n",
       "      <td>0.905004</td>\n",
       "      <td>-0.519088</td>\n",
       "      <td>2.868947</td>\n",
       "      <td>will</td>\n",
       "      <td>not dancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22998</th>\n",
       "      <td>1049</td>\n",
       "      <td>0.808892</td>\n",
       "      <td>-0.081096</td>\n",
       "      <td>2.680727</td>\n",
       "      <td>0.543384</td>\n",
       "      <td>-0.763010</td>\n",
       "      <td>2.796647</td>\n",
       "      <td>0.771182</td>\n",
       "      <td>-0.748278</td>\n",
       "      <td>2.809686</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.137862</td>\n",
       "      <td>3.231271</td>\n",
       "      <td>0.422788</td>\n",
       "      <td>-0.496354</td>\n",
       "      <td>2.734586</td>\n",
       "      <td>0.905004</td>\n",
       "      <td>-0.519088</td>\n",
       "      <td>2.868947</td>\n",
       "      <td>will</td>\n",
       "      <td>not dancing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         50  1.3046719999999998  -0.38843049999999996  3.393734  0.8746868  \\\n",
       "22994  1045            0.805225             -0.064882  2.703505   0.529117   \n",
       "22995  1046            0.807749             -0.072318  2.692815   0.536634   \n",
       "22996  1047            0.807749             -0.072318  2.692815   0.536634   \n",
       "22997  1048            0.808892             -0.081096  2.680727   0.543384   \n",
       "22998  1049            0.808892             -0.081096  2.680727   0.543384   \n",
       "\n",
       "       -0.6795787  3.3568839999999995  0.8905584999999999  -0.6281642  \\\n",
       "22994   -0.748536            2.817768            0.766553   -0.729576   \n",
       "22995   -0.751952            2.808611            0.769255   -0.738360   \n",
       "22996   -0.751952            2.808611            0.769255   -0.738360   \n",
       "22997   -0.763010            2.796647            0.771182   -0.748278   \n",
       "22998   -0.763010            2.796647            0.771182   -0.748278   \n",
       "\n",
       "       3.383114     ...       -1.462777  3.551411  1.055417  \\\n",
       "22994  2.835117     ...       -1.123607  3.249849  0.427156   \n",
       "22995  2.822912     ...       -1.132008  3.242096  0.423794   \n",
       "22996  2.822912     ...       -1.132008  3.242096  0.423794   \n",
       "22997  2.809686     ...       -1.137862  3.231271  0.422788   \n",
       "22998  2.809686     ...       -1.137862  3.231271  0.422788   \n",
       "\n",
       "       -0.7448916999999999  3.120533  1.0443719999999999  -0.6931599  \\\n",
       "22994            -0.474788  2.765285            0.902814   -0.500578   \n",
       "22995            -0.483205  2.754180            0.904506   -0.510160   \n",
       "22996            -0.483205  2.754180            0.904506   -0.510160   \n",
       "22997            -0.496354  2.734586            0.905004   -0.519088   \n",
       "22998            -0.496354  2.734586            0.905004   -0.519088   \n",
       "\n",
       "       3.1474599999999997  alden      dancing  \n",
       "22994            2.897322   will  not dancing  \n",
       "22995            2.882418   will  not dancing  \n",
       "22996            2.882418   will  not dancing  \n",
       "22997            2.868947   will  not dancing  \n",
       "22998            2.868947   will  not dancing  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "x_train = pd.read_csv('training_data.csv', sep=',',header=1)\n",
    "x_train.values\n",
    "\n",
    "x_train.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_8 to have shape (1,) but got array with shape (10,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ffd7649a558e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_8 to have shape (1,) but got array with shape (10,)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "max_features = 1024\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, output_dim=256))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=16, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
